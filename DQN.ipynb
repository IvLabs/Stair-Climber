{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
    "    plt.plot(rewards)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    \n",
    "    def __init__(self,max_mem,batch_size,state_dims,action_dims):\n",
    "        \n",
    "        self.max_mem = max_mem\n",
    "        self.batch_size = batch_size\n",
    "        self.mem_ctr = 0\n",
    "        self.mem_full_flag = False\n",
    "        \n",
    "        self.state = np.zeros((self.max_mem,state_dims))\n",
    "        self.action = np.zeros((self.max_mem))\n",
    "        self.reward = np.zeros((self.max_mem))\n",
    "        self.next_state = np.zeros((self.max_mem,state_dims))\n",
    "        self.done = np.zeros((self.max_mem))\n",
    "        \n",
    "    \n",
    "    def store(self,state,action,reward,next_state,done):\n",
    "        \n",
    "        if self.mem_ctr==self.max_mem:\n",
    "            self.mem_full_flag = True\n",
    "            self.mem_ctr = self.mem_ctr%self.memory\n",
    "            \n",
    "        self.state[self.mem_ctr] = state\n",
    "        self.action[self.mem_ctr] = action\n",
    "        self.reward[self.mem_ctr] = reward\n",
    "        self.next_state[self.mem_ctr] = next_state\n",
    "        self.done[self.mem_ctr] = done\n",
    "        \n",
    "        self.mem_ctr += 1\n",
    "    \n",
    "    def sample(self):\n",
    "        if self.mem_full_flag:\n",
    "            current_mem = self.max_mem\n",
    "        else:\n",
    "            current_mem = self.mem_ctr\n",
    "        \n",
    "        batch = np.random.choice(current_mem, self.batch_size, replace = False)\n",
    "        state_batch = self.state[batch]\n",
    "        action_batch = self.action[batch]\n",
    "        reward_batch = self.reward[batch]\n",
    "        done_batch = self.done[batch]\n",
    "        next_state_batch = self.next_state[batch]\n",
    "        \n",
    "        return state_batch,action_batch,reward_batch,next_state_batch,done_batch\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.mem_full_flag:\n",
    "            return self.max_mem\n",
    "        else:\n",
    "            return self.mem_ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self,l_r,input_dims,n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(input_dims,128)\n",
    "        self.linear2 = nn.Linear(128,128)\n",
    "        self.action_value = nn.Linear(128,n_actions)\n",
    "        \n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr = l_r)\n",
    "        \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = torch.relu(self.linear1(x))\n",
    "        x = torch.relu(self.linear2(x))\n",
    "        action_values = self.action_value(x)    \n",
    "            \n",
    "        return action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    def __init__(self,epsilon,eps_decay,epsilon_min,gamma,l_r,state_dims,action_dims,max_mem,batch_size,target_update,env):\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.eps_decay = eps_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.gamma = gamma\n",
    "        self.env = env\n",
    "        \n",
    "        self.target_update = target_update\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.memory = Memory(max_mem,batch_size,state_dims,action_dims)\n",
    "        \n",
    "        self.Q_net = DQN(l_r,state_dims,action_dims)\n",
    "        self.targetQ_net = DQN(l_r,state_dims,action_dims)\n",
    "        \n",
    "    def store(self,state,action,reward,next_state,terminal):\n",
    "        self.memory.store(state,action,reward,next_state,terminal)\n",
    "    \n",
    "    def epsilon_greedy(self,state):\n",
    "        r = np.random.random()\n",
    "        state = torch.Tensor(state).to(self.Q_net.device)\n",
    "        if r<self.epsilon:\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_val = self.Q_net.forward(state)\n",
    "                action = torch.argmax(q_val).item()\n",
    "        return action\n",
    "    \n",
    "    def epsilon_decay(self):\n",
    "        if self.epsilon>self.epsilon_min:\n",
    "            self.epsilon = self.epsilon-self.eps_decay\n",
    "        return self.epsilon\n",
    "    \n",
    "    def update_target(self):\n",
    "        for target_param, param in zip(self.targetQ_net.parameters(), self.Q_net.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "    \n",
    "    def improve(self):\n",
    "        \n",
    "        if (len(self.memory)<self.batch_size):\n",
    "            return\n",
    "        \n",
    "        state,action,reward,next_state,done = self.memory.sample()\n",
    "        batch_index = np.arange(self.batch_size)\n",
    "        \n",
    "        state      = torch.FloatTensor(state).to(self.Q_net.device)\n",
    "        reward     = torch.FloatTensor(reward).to(self.Q_net.device)\n",
    "        next_state = torch.FloatTensor(next_state).to(self.Q_net.device)\n",
    "        done       = torch.FloatTensor(done).to(self.Q_net.device)\n",
    "        '''\n",
    "        q_val = self.Q_net.forward(state)\n",
    "        q_next = self.targetQ_net.forward(next_state).detach()\n",
    "    \n",
    "        action_values = torch.max(q_next,1)[0].unsqueeze(1)\n",
    "\n",
    "        \n",
    "        q_val = q_val[batch_index,np.array(action)].unsqueeze(1)\n",
    "        q_estimate = reward + self.gamma*action_values*(1-done)\n",
    "        '''\n",
    "        \n",
    "        q_val = self.Q_net.forward(state)\n",
    "        q_next = self.targetQ_net.forward(next_state) \n",
    "        #q_target = self.Q_net.forward(state).detach()  \n",
    "        q_target = q_val.clone().detach()\n",
    "        \n",
    "        action_values = torch.max(q_next,1)[0]\n",
    "        #print(action_values.shape)\n",
    "        q_target[batch_index, action] = reward + self.gamma*action_values*(1-done)\n",
    "\n",
    "        \n",
    "        loss = self.Q_net.criterion(q_val,q_target).to(self.Q_net.device)\n",
    "            \n",
    "        self.Q_net.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.Q_net.optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent =  DQNAgent(epsilon=1,eps_decay=0.05,epsilon_min=0.01,gamma=0.99,l_r=0.0003,state_dims=4,action_dims=2,\n",
    "                max_mem=100000,batch_size=32,target_update=10,env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_idx   = 0\n",
    "max_steps   = 200\n",
    "rewards = []\n",
    "n_games = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.0\n",
      "8.0\n",
      "16.0\n",
      "22.0\n",
      "18.0\n",
      "11.0\n",
      "13.0\n",
      "9.0\n",
      "16.0\n",
      "9.0\n",
      "11.0\n",
      "18.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "13.0\n",
      "11.0\n",
      "12.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "8.0\n",
      "9.0\n",
      "9.0\n",
      "8.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "8.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "8.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "11.0\n",
      "10.0\n",
      "12.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "11.0\n",
      "9.0\n",
      "10.0\n",
      "12.0\n",
      "11.0\n",
      "13.0\n",
      "12.0\n",
      "11.0\n",
      "9.0\n",
      "14.0\n",
      "17.0\n",
      "11.0\n",
      "14.0\n",
      "45.0\n",
      "70.0\n",
      "61.0\n",
      "31.0\n",
      "36.0\n",
      "21.0\n",
      "17.0\n",
      "14.0\n",
      "20.0\n",
      "19.0\n",
      "12.0\n",
      "13.0\n",
      "52.0\n",
      "88.0\n",
      "62.0\n",
      "200.0\n",
      "57.0\n",
      "67.0\n",
      "68.0\n",
      "178.0\n",
      "158.0\n",
      "200.0\n",
      "119.0\n",
      "200.0\n",
      "135.0\n",
      "181.0\n",
      "149.0\n",
      "185.0\n",
      "176.0\n",
      "143.0\n",
      "146.0\n",
      "177.0\n",
      "124.0\n",
      "154.0\n",
      "200.0\n",
      "200.0\n",
      "184.0\n",
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "179.0\n",
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "200.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-23d8fee8e84b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimprove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-24fdef09a59f>\u001b[0m in \u001b[0;36mimprove\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(n_games):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for s in range(max_steps):\n",
    "        action = agent.epsilon_greedy(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        agent.store(state, action, reward, next_state, done)\n",
    "        agent.improve()\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "        if i % agent.target_update == 0:\n",
    "            agent.update_target()\n",
    "    rewards.append(episode_reward)\n",
    "    agent.epsilon_decay()\n",
    "    print(episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
